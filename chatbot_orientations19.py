import streamlit as st 
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# Charger les embeddings et le vectorstore sauvegard√©
embeddings = OpenAIEmbeddings(openai_api_key="votre_cl√©_api_openai")
vectorstore = Chroma(persist_directory="chroma_db18", embedding_function=embeddings)
retriever = vectorstore.as_retriever()

# Initialiser le mod√®le de chat (par exemple GPT-4) avec la cl√© API
llm = ChatOpenAI(model_name="gpt-4", openai_api_key="votre_cl√©_api_openai")

# Cr√©er la cha√Æne de questions-r√©ponses avec le mod√®le et le retriever
qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

# Interface utilisateur avec Streamlit
st.title("üéì Chatbot Orientation Universitaire")
st.write("Posez vos questions concernant les formations et universit√©s !")

# Cr√©er un espace vide pour la r√©ponse
response_placeholder = st.empty()

# Demander une question √† l'utilisateur
user_query = st.text_input("Votre question :")

# Si l'utilisateur a pos√© une question, g√©n√©rer une r√©ponse et l'afficher au-dessus de la zone de saisie
if user_query:
    response_placeholder.write(f"üë§   {user_query}")
    
    # Obtenir la r√©ponse du mod√®le
    response = qa_chain.run(user_query)
    
    # Afficher la r√©ponse du chatbot au-dessus de la barre de saisie
    response_placeholder.write(f"ü§ñ   {response}")
